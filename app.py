import os, io, re, time, tempfile
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed

import requests
import pandas as pd
import numpy as np
from openai import OpenAI
import streamlit as st

# ---- OpenAI (Responses API) ----

api_key = st.secrets.get("OPENAI_API_KEY", os.getenv("OPENAI_API_KEY"))
if not api_key:
    st.error("Missing OPENAI_API_KEY (set env var or add to Streamlit secrets).")
    st.stop()

client = OpenAI(api_key=api_key)

with st.expander("üîç OpenAI connection diagnostics", expanded=False):
    key_src = "st.secrets" if "OPENAI_API_KEY" in st.secrets else "env"
    mask = lambda s: (s[:7] + "..." + s[-4:]) if s and len(s) > 12 else "unset"
    st.write("Key source:", key_src)
    st.write("API key (masked):", mask(st.secrets.get("OPENAI_API_KEY") or os.getenv("OPENAI_API_KEY")))
    try:
        _ = client.models.list()
        st.success("Models list ok ‚Äî auth + project look good.")
    except Exception as e:
        st.error(f"Models list failed: {e}")
    try:
        r = client.responses.create(model="gpt-4.1-mini", input="ping")
        st.success("Responses call ok.")
    except Exception as e:
        st.error(f"Responses call failed: {e}")

# =========================================
# Streamlit UI
# =========================================
st.set_page_config(page_title="BSE Company Update ‚Äî OpenAI PDF Summarizer", layout="wide")
st.title("üìà BSE Company Update ‚Äî M&A / Merger / Scheme / JV (OpenAI-only)")
st.caption("Fetch BSE announcements ‚Üí pick PDF ‚Üí upload to OpenAI ‚Üí summarize. No local NLP or OCR is used; summaries are generated by OpenAI models.")

# =========================================
# Small utilities
# =========================================
_ILLEGAL_RX = re.compile(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]')
def _clean(s: str) -> str:
    return _ILLEGAL_RX.sub('', s) if isinstance(s, str) else s

def _first_col(df: pd.DataFrame, names):
    for n in names:
        if n in df.columns: return n
    return None

def _norm(s):
    return re.sub(r"\s+", " ", str(s or "")).strip()

def _key(s: str) -> str:
    return _norm(s).lower()

def _slug(s: str, maxlen: int = 60) -> str:
    s = re.sub(r"[^A-Za-z0-9]+", "_", str(s or "")).strip("_")
    return (s[:maxlen] if len(s) > maxlen else s) or "file"

def _fmt(d: datetime.date) -> str:
    return d.strftime("%Y%m%d")

def _df_to_excel_bytes(df: pd.DataFrame) -> bytes | None:
    """
    Return XLSX bytes if possible; None if engines unavailable.
    """
    try:
        bio = io.BytesIO()
        try:
            with pd.ExcelWriter(bio, engine="xlsxwriter") as writer:
                df.to_excel(writer, index=False, sheet_name="Data")
        except Exception:
            # fall back to default engine (e.g., openpyxl)
            with pd.ExcelWriter(bio) as writer:
                df.to_excel(writer, index=False, sheet_name="Data")
        bio.seek(0)
        return bio.getvalue()
    except Exception:
        return None

# --- classify & prompts -------------------------------------------------
def _lower_join(*parts) -> str:
    return " ".join([str(p or "") for p in parts]).lower()

_ORDER_KWS = (
    "order", "purchase order", "contract", "loa", "loi", "bagged",
    "work order", "tender", "award", "awarded", "received order",
)
_CAPEX_KWS = (
    "capex", "capital expenditure", "expansion", "capacity", "debottleneck",
    "greenfield", "brownfield", "plant", "facility", "commission", "commissioning",
    "new unit", "capability expansion", "project cost"
)
_RESULT_KWS = (
    "financial results", "results", "q1", "q2", "q3", "q4", "quarter",
    "half year", "h1", "h2", "year ended", "audited", "unaudited",
    "outcome of board meeting"
)

def classify_announcement(category: str, subcat: str, headline: str) -> str:
    s = _lower_join(category, subcat, headline)
    if any(k in s for k in _ORDER_KWS):  return "order"
    if any(k in s for k in _CAPEX_KWS):  return "capex"
    if any(k in s for k in _RESULT_KWS): return "result"
    return "special"

def build_task_prompt(company: str, headline: str, category: str, subcat: str, kind: str, style: str) -> str:
    if kind == "order":
        return f"""
You are an equity analyst. Read the attached PDF (BSE filing).
Company: {company or 'NA'}
Category/Subcategory: {category or 'NA'} / {subcat or 'NA'}
Headline: {headline or 'NA'}

Return a concise bullet summary that EXTRACTS (do not guess):
- Order amount (value + currency). If multiple orders, list each and a total.
- Customer/buyer (name), product/service scope, geography/site.
- Execution timeline (start‚Äìend dates or total months). If only tenure is given, state it.
- Revenue recognition timeline (any FY/quarter split if disclosed).
- Impact on order book/backlog (if disclosed).
If a field is absent, write "Not disclosed".
Conclude with one line on near-term revenue visibility (only if explicitly disclosed).
"""
    if kind == "capex":
        return f"""
You are an equity analyst. Read the attached PDF (BSE filing).
Company: {company or 'NA'}
Category/Subcategory: {category or 'NA'} / {subcat or 'NA'}
Headline: {headline or 'NA'}

Return a concise bullet summary that EXTRACTS (do not guess):
- Capex outlay (value + currency), purpose, location.
- Capacity addition (units), base vs post capacity if disclosed.
- Commissioning/start timeline (milestones/phases).
- Funding mix (internal accruals/debt/equity).
- Expected revenue realization/ramp (annual run-rate or guidance, if disclosed).
- Margin/ROCE/payback or approvals (if disclosed).
If a field is absent, write "Not disclosed".
"""
    if kind == "result":
        return f"""
You are an equity analyst. Read the attached PDF (BSE financial results/press release).
Company: {company or 'NA'}
Headline: {headline or 'NA'}

Identify the period reported (e.g., Q1 FY26) and present:
- Revenue from Operations: value; YoY %; QoQ %.
- EBITDA: value; YoY %; QoQ %; EBITDA margin % (and YoY/QoQ delta if available).
- PAT: value; YoY %; QoQ %; PAT margin % (and YoY/QoQ delta if available).
If base-period numbers are not present, write "Not disclosed" for that comparison.
Use only figures from the PDF; do not infer.
"""
    # special/default ‚Äì original M&A-style prompt
    return f"""
You are an M&A analyst. Read the attached PDF (BSE filing).
Company: {company or 'NA'}
Subcategory: {subcat or 'NA'}
Headline: {headline or 'NA'}

Return a crisp {('bullet point summary (7‚Äì10 bullets)' if style=='bullets' else 'executive narrative (120‚Äì180 words)')}
that covers: deal type, consideration/currency, stake/valuation, key dates/conditions (SEBI/CCI/NCLT etc.), strategic rationale, financial impact/guidance, and next steps.
If data is not present, explicitly say "Not disclosed" rather than guessing.
"""

# =========================================
# Attachment URL candidates (unchanged)
# =========================================
def _candidate_urls(row):
    cands = []
    att = str(row.get("ATTACHMENTNAME") or "").strip()
    if att:
        cands += [
            f"https://www.bseindia.com/xml-data/corpfiling/AttachHis/{att}",
            f"https://www.bseindia.com/xml-data/corpfiling/Attach/{att}",
            f"https://www.bseindia.com/xml-data/corpfiling/AttachLive/{att}",
        ]
    ns = str(row.get("NSURL") or "").strip()
    if ".pdf" in ns.lower():
        cands.append(ns if ns.lower().startswith("http") else "https://www.bseindia.com/" + ns.lstrip("/"))
    seen, out = set(), []
    for u in cands:
        if u and u not in seen:
            out.append(u); seen.add(u)
    return out

# =========================================
# BSE fetch ‚Äî strict; returns raw DF (no filtering), day-by-day
# =========================================
def fetch_bse_announcements_strict(start_yyyymmdd: str,
                                   end_yyyymmdd: str,
                                   verbose: bool = True,
                                   request_timeout: int = 25) -> pd.DataFrame:
    """Fetches raw announcements across a date range by looping day-by-day.
    NOTE: No deduplication; results from each day are appended as-is. No category/subcategory filtering here.
    """
    assert len(start_yyyymmdd) == 8 and len(end_yyyymmdd) == 8
    assert start_yyyymmdd <= end_yyyymmdd

    base_page = "https://www.bseindia.com/corporates/ann.html"
    url = "https://api.bseindia.com/BseIndiaAPI/api/AnnSubCategoryGetData/w"

    s = requests.Session()
    s.headers.update({
        "User-Agent": "Mozilla/5.0",
        "Accept": "application/json, text/plain, */*",
        "Accept-Language": "en-US,en;q=0.9",
        "Referer": base_page,
        "X-Requested-With": "XMLHttpRequest",
        "Cache-Control": "no-cache",
        "Pragma": "no-cache",
    })

    try:
        s.get(base_page, timeout=15)
    except Exception:
        pass

    variants = [
        {"subcategory": "-1", "strSearch": "P"},
        {"subcategory": "-1", "strSearch": ""},
        {"subcategory": "",   "strSearch": "P"},
        {"subcategory": "",   "strSearch": ""},
    ]

    start_dt = datetime.strptime(start_yyyymmdd, "%Y%m%d").date()
    end_dt   = datetime.strptime(end_yyyymmdd, "%Y%m%d").date()

    all_rows = []
    cur = start_dt
    while cur <= end_dt:
        day_str = cur.strftime("%Y%m%d")
        day_rows = []
        for v in variants:
            params = {
                "pageno": 1, "strCat": "-1", "subcategory": v["subcategory"],
                "strPrevDate": day_str, "strToDate": day_str,
                "strSearch": v["strSearch"], "strscrip": "", "strType": "C",
            }
            rows, total, page = [], None, 1
            while True:
                r = s.get(url, params=params, timeout=request_timeout)
                ct = r.headers.get("content-type","")
                if "application/json" not in ct:
                    if verbose:
                        try:
                            st.warning(f"[{day_str} variant {v}] non-JSON on page {page} (ct={ct}).")
                        except Exception:
                            pass
                    break
                data = r.json()
                table = data.get("Table") or []
                rows.extend(table)
                if total is None:
                    try:
                        total = int((data.get("Table1") or [{}])[0].get("ROWCNT") or 0)
                    except Exception:
                        total = None
                if not table: break
                params["pageno"] += 1; page += 1; time.sleep(0.25)
                if total and len(rows) >= total: break
            if rows:
                day_rows = rows
                break
        if day_rows:
            all_rows.extend(day_rows)
        cur += timedelta(days=1)

    if not all_rows:
        return pd.DataFrame()

    all_keys = set()
    for r in all_rows: all_keys.update(r.keys())
    df = pd.DataFrame(all_rows, columns=list(all_keys))
    return df

# =========================================
# OpenAI PDF summarization
# =========================================
def _download_pdf(url: str, timeout=25) -> bytes:
    s = requests.Session()
    s.headers.update({
        "User-Agent": "Mozilla/5.0",
        "Accept": "application/pdf,application/octet-stream,*/*",
        "Referer": "https://www.bseindia.com/corporates/ann.html",
        "Accept-Language": "en-US,en;q=0.9",
    })
    r = s.get(url, timeout=timeout, allow_redirects=True, stream=False)
    if r.status_code != 200:
        raise RuntimeError(f"HTTP {r.status_code}")
    data = r.content
    if not (data[:8].startswith(b"%PDF") or url.lower().endswith(".pdf")):
        pass
    return data

def _upload_to_openai(pdf_bytes: bytes, fname: str = "document.pdf"):
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
        tmp.write(pdf_bytes)
        tmp.flush()
        f = client.files.create(file=open(tmp.name, "rb"), purpose="assistants")
    return f

def summarize_pdf_with_openai(pdf_bytes: bytes, company: str, headline: str, subcat: str,
                              model: str = "gpt-4.1-mini", style: str = "bullets", max_output_tokens: int = 800,
                              temperature: float = 0.2, task_override: str | None = None) -> str:
    f = _upload_to_openai(pdf_bytes, fname=f"{_slug(company or 'doc')}.pdf")
    task = task_override or f"""
You are an M&A analyst. Read the attached PDF (BSE filing).
Company: {company or 'NA'}
Subcategory: {subcat or 'NA'}
Headline: {headline or 'NA'}

Return a crisp {('bullet point summary (7‚Äì10 bullets)' if style=='bullets' else 'executive narrative (120‚Äì180 words)')}
that covers: deal type, consideration/currency, stake/valuation, key dates/conditions (SEBI/CCI/NCLT etc.), strategic rationale, financial impact/guidance, and next steps.
If data is not present, explicitly say "Not disclosed" rather than guessing.
"""
    resp = client.responses.create(
        model=model,
        temperature=temperature,
        max_output_tokens=max_output_tokens,
        input=[{
            "role": "user",
            "content": [
                {"type": "input_text", "text": task},
                {"type": "input_file", "file_id": f.id},
            ],
        }],
    )
    return (resp.output_text or "").strip()

def safe_summarize(*args, **kwargs) -> str:
    for i in range(4):
        try:
            return summarize_pdf_with_openai(*args, **kwargs)
        except Exception as e:
            msg = str(e)
            if "429" in msg or "rate" in msg.lower():
                time.sleep(2.0 * (i + 1))
                continue
            raise
    return "‚ö†Ô∏è Unable to summarize due to repeated rate limits."

# =========================================
# Sidebar controls
# =========================================
with st.sidebar:
    st.header("‚öôÔ∏è Controls")
    today = datetime.now().date()
    start_date = st.date_input("Start date", value=today - timedelta(days=1), max_value=today)
    end_date   = st.date_input("End date", value=today, max_value=today, min_value=start_date)

    model = st.selectbox(
        "OpenAI model",
        ["gpt-4.1-mini", "gpt-4o-mini", "gpt-4.1"],
        index=0,
        help="Models with vision/file-reading capability. 4.1-mini/4o-mini are cost-efficient."
    )
    style = st.radio("Summary style", ["bullets", "narrative"], horizontal=True)
    max_tokens = st.slider("Max output tokens", 200, 2000, 800, step=50)
    temperature = st.slider("Creativity (temperature)", 0.0, 1.0, 0.2, step=0.1)

    max_workers = st.slider("Parallel summaries", 1, 8, 3, help="Lower if you hit 429s.")
    max_items = st.slider("Max announcements to summarize", 5, 200, 60, step=5)

    run = st.button("üöÄ Step 1: Fetch announcements", type="primary")

# =========================================
# Run pipeline (fetch ‚Üí user dropdown filters ‚Üí PDFs ‚Üí OpenAI summaries)
# =========================================
def _pick_cols(df: pd.DataFrame):
    nm = _first_col(df, ["SLONGNAME","SNAME","SC_NAME","COMPANYNAME"]) or "SLONGNAME"
    subcol = _first_col(df, ["SUBCATEGORYNAME","SUBCATEGORY","SUB_CATEGORY","NEWS_SUBCATEGORY","NEWS_SUB"]) or "SUBCATEGORYNAME"
    catcol = _first_col(df, ["CATEGORYNAME","CATEGORY","NEWS_CAT","NEWSCATEGORY","NEWS_CATEGORY"])
    return nm, subcol, catcol

# Session state for persistence
if "latest_df" not in st.session_state:
    st.session_state["latest_df"] = pd.DataFrame()
if "sel_cats" not in st.session_state:
    st.session_state["sel_cats"] = []
if "sel_subs" not in st.session_state:
    st.session_state["sel_subs"] = []
if "prev_sel_cats" not in st.session_state:
    st.session_state["prev_sel_cats"] = []

if run:
    if not os.getenv("OPENAI_API_KEY"):
        st.error("Missing OPENAI_API_KEY (set env var, add to Streamlit Secrets, or export in your shell).")
        st.stop()

    start_str, end_str = _fmt(start_date), _fmt(end_date)

    with st.status("Fetching announcements‚Ä¶", expanded=True):
        df_hits = fetch_bse_announcements_strict(start_str, end_str, verbose=False)
        st.write(f"Fetched rows: **{len(df_hits)}**")

    st.session_state["latest_df"] = df_hits

df_hits = st.session_state["latest_df"]

if df_hits.empty:
    st.info("Pick your date range and click **Step 1: Fetch announcements**. Then choose Category/Subcategory and summarize.")
else:
    st.subheader("üîé Step 2: Choose filters")

    nm, subcol, catcol = _pick_cols(df_hits)

    # 1) Category options (normalized, guarded)
    if catcol and catcol in df_hits.columns:
        cats_all_keys = sorted(df_hits[catcol].astype(str).map(_key).dropna().unique())
    else:
        cats_all_keys = []
        st.info("No Category column detected in the fetched data.", icon="‚ÑπÔ∏è")

    sel_cats = st.multiselect(
        "Category (leave blank for all)",
        options=cats_all_keys,
        default=[c for c in st.session_state.get("sel_cats", []) if c in cats_all_keys],
        key="sel_cats"
    )

    # Reset subs if categories changed
    if st.session_state["prev_sel_cats"] != sel_cats:
        st.session_state["sel_subs"] = []
    st.session_state["prev_sel_cats"] = sel_cats

    # 2) Subcategory options derived AFTER category filter
    df_for_subs = df_hits.copy()
    if catcol and catcol in df_for_subs.columns and sel_cats:
        df_for_subs["_cat_key"] = df_for_subs[catcol].astype(str).map(_key)
        df_for_subs = df_for_subs[df_for_subs["_cat_key"].isin(sel_cats)].drop(columns=["_cat_key"])

    if subcol and subcol in df_for_subs.columns:
        subs_filtered_keys = sorted(df_for_subs[subcol].astype(str).map(_key).dropna().unique())
    else:
        subs_filtered_keys = []
        st.info("No Subcategory column detected in the fetched data (or none after Category filter).", icon="‚ÑπÔ∏è")

    sel_subs = st.multiselect(
        "Subcategory (leave blank for all)",
        options=subs_filtered_keys,
        default=[s for s in st.session_state.get("sel_subs", []) if s in subs_filtered_keys],
        key="sel_subs",
        help="Options refresh based on chosen Category."
    )

    # 3) Apply both filters (normalized)
    df_filtered = df_hits.copy()

    if catcol and catcol in df_filtered.columns and sel_cats:
        df_filtered["_cat_key"] = df_filtered[catcol].astype(str).map(_key)
        df_filtered = df_filtered[df_filtered["_cat_key"].isin(sel_cats)].drop(columns=["_cat_key"])

    if subcol and subcol in df_filtered.columns and sel_subs:
        df_filtered["_sub_key"] = df_filtered[subcol].astype(str).map(_key)
        df_filtered = df_filtered[df_filtered["_sub_key"].isin(sel_subs)].drop(columns=["_sub_key"])

    st.write(f"After filters: **{len(df_filtered)}** rows")

    # --- Download buttons (Excel + CSV) ---
    col_dl1, col_dl2 = st.columns([1,1])
    with col_dl1:
        xbytes = _df_to_excel_bytes(df_filtered)
        if xbytes is not None:
            st.download_button(
                "‚¨áÔ∏è Download filtered table (Excel)",
                data=xbytes,
                file_name=f"bse_filtered_{_fmt(start_date)}_{_fmt(end_date)}.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                use_container_width=True
            )
        else:
            st.warning("Excel writer not available. Use CSV download instead.", icon="‚ö†Ô∏è")
    with col_dl2:
        st.download_button(
            "‚¨áÔ∏è Download filtered table (CSV)",
            data=df_filtered.to_csv(index=False).encode("utf-8"),
            file_name=f"bse_filtered_{_fmt(start_date)}_{_fmt(end_date)}.csv",
            mime="text/csv",
            use_container_width=True
        )

    st.dataframe(df_filtered.head(50), use_container_width=True)

    summarize_btn = st.button("üß† Step 3: Apply filters & Summarize", type="secondary")

    if summarize_btn:
        if df_filtered.empty:
            st.warning("No rows after filters.")
            st.stop()

        df_to_sum = df_filtered.head(max_items)

        # Build list of PDF targets
        rows = []
        for _, r in df_to_sum.iterrows():
            urls = _candidate_urls(r)
            rows.append((r, urls))

        st.subheader("üìë Summaries (OpenAI)")

        def worker(idx, row, urls):
            # try urls in order until one downloads
            pdf_bytes, used_url = None, ""
            for u in urls:
                try:
                    data = _download_pdf(u, timeout=25)
                    if data and len(data) > 500:
                        pdf_bytes, used_url = data, u
                        break
                except Exception:
                    continue
            if not pdf_bytes:
                return idx, used_url, "‚ö†Ô∏è Could not fetch a valid PDF.", None

            company = str(row.get(_first_col(df_hits, ["SLONGNAME","SNAME","SC_NAME","COMPANYNAME"]) or "SLONGNAME") or "").strip()
            headline = str(row.get("HEADLINE") or "").strip()
            subcat_val = str(row.get(subcol) or "").strip() if subcol else ""
            category_val = str(row.get(catcol) or "").strip() if catcol else ""

            kind = classify_announcement(category_val, subcat_val, headline)
            task_text = build_task_prompt(company, headline, category_val, subcat_val, kind, style)

            summary = safe_summarize(
                pdf_bytes, company, headline, subcat_val,
                model=model,
                style=("bullets" if style=="bullets" else "narrative"),
                max_output_tokens=int(max_tokens),
                temperature=float(temperature),
                task_override=task_text
            )
            return idx, used_url, summary, kind

        with ThreadPoolExecutor(max_workers=max_workers) as ex:
            futs = [ex.submit(worker, i, r, urls) for i, (r, urls) in enumerate(rows)]
            for fut in as_completed(futs):
                i, pdf_url, summary, kind = fut.result()
                r = rows[i][0]
                company = str(r.get(_first_col(df_hits, ["SLONGNAME","SNAME","SC_NAME","COMPANYNAME"]) or "SLONGNAME") or "").strip()
                dt = str(r.get("NEWS_DT") or "").strip()
                subcat_val = str(r.get(subcol) or "").strip() if subcol else ""
                headline = str(r.get("HEADLINE") or "").strip()

                badge = {"order": "üü¶ Order book", "capex": "üüß Capex", "result": "üü© Results", "special": "‚¨ú Special"}[kind]
                with st.expander(f"{badge} ‚Ä¢ {company or 'Unknown'} ‚Äî {dt} ‚Ä¢ {subcat_val or 'N/A'}", expanded=False):
                    if headline:
                        st.markdown(f"**Headline:** {headline}")
                    if pdf_url:
                        st.markdown(f"[PDF link]({pdf_url})")
                    st.markdown(summary)
